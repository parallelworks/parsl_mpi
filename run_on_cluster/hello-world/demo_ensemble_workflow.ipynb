{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a21bdaa-9df8-4503-bfc0-39e2d7efa766",
   "metadata": {},
   "source": [
    "# Demonstration Parsl workflow for multiple ensemble members\n",
    "\n",
    "This notebook is a stand-alone sandbox to explore a Parsl workflow that sets up multiple ensemble member apps that depend on initial and final apps. This notebook is designed to be run directly on an HPC resource.\n",
    "\n",
    "## Workflow visualization\n",
    "\n",
    "There are three options for visualizing a Parsl workflow:\n",
    "1. Manual \"direct\" launch of `parsl-visualize` before workflow runs;\n",
    "2. Visualization launched as part of the workflow (i.e. self-launch); and\n",
    "3. Offline visualization (i.e. for browsing previous Parsl workflows or starting visualization completely independently of the workflow).\n",
    "\n",
    "Examples for all three are provided below but options \\#1 and \\#3 are commented out.\n",
    "\n",
    "## Interactive usage\n",
    "\n",
    "This notebook can be used via a `JupyterLab` interactive session on a cluster head node. The visualization can be accessed via a `Desktop` interactive session on the same cluster head node. You will probably need to bootstrap the `parsl` Conda environment first (see below), select the `parsl` kernel for the notebook, and then restart the notebook to use Parsl functionality.\n",
    "\n",
    "## Workflow parameters\n",
    "\n",
    "The key customizable parameters for this workflow are defined immediately below. For a fully automated workflow (i.e. non-interactive workflow in `main.py`), these parameters are typically specified in the workflow launch form (and corresponding `.json` package for API launch) and then they make it to the command line launch of `main.py` on the head node of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafad227-6bc2-4f02-9fd2-eb705e18abe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Workflow parameters\n",
    "\n",
    "# Bootstrap installation info\n",
    "install=False\n",
    "install_from_scratch=False\n",
    "conda_base_path=\"~/pw/software/.miniconda3c/\"\n",
    "conda_env_name=\"parsl\"\n",
    "\n",
    "# App workdir path info\n",
    "param_log_dir='./parsl-app-logs'\n",
    "\n",
    "# Ensemble size and other solar forcing parameters\n",
    "param_ens_size=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768cea9-9406-4d21-8d60-2d78ff2b9182",
   "metadata": {},
   "source": [
    "## Installs\n",
    "\n",
    "There are two install options:\n",
    "1. `install_from_scratch = True` documents the steps to build a particular environment\n",
    "2. `install_from_scratch = False` is faster to reconstruct a Conda environment from an exported env file `.yaml` than to rebuild from scratch and promotes reproducibility.\n",
    "\n",
    "The reconstruction command is kept active here since \n",
    "env files are distributed with this notebook. Once the command to reconstruct\n",
    "the Conda environment has been run, you may need to tell\n",
    "this notebook to use the kernel from that Conda environment\n",
    "with the `Kernel > Change kernel...` option in the menu above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34799ce1-6bf0-4862-a4ae-f0f089701bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You don't need to rerun the install if the environment has already\n",
    "# been built and selected as the kernel for this notebook.\n",
    "if (install):\n",
    "    if (install_from_scratch):\n",
    "        \n",
    "        # Currently there is a dependency bug with ipykernel and Python 3.13,\n",
    "        # so pin to a different Python.\n",
    "        ! conda create -y --name {conda_env_name} python=3.9\n",
    "        \n",
    "        # To use a Jupyter notebook with a\n",
    "        # specific conda environment:\n",
    "        ! conda install -y --name {conda_env_name} requests\n",
    "        ! conda install -y --name {conda_env_name} ipykernel\n",
    "        ! conda install -y --name {conda_env_name} -c anaconda jinja2\n",
    "        \n",
    "        # pip installs\n",
    "        # Conda does not install monitoring, so use pip\n",
    "        # Each Conda env has its own pip, so need to activate.\n",
    "        ! source {conda_base_path}/etc/profile.d/conda.sh; conda activate {conda_env_name}; pip install --upgrade pip\n",
    "        ! source {conda_base_path}/etc/profile.d/conda.sh; conda activate {conda_env_name}; pip install 'parsl[monitoring, visualization]'\n",
    "        \n",
    "        # The environment was then exported with:\n",
    "        ! conda env export --name {conda_env_name} > ./requirements/{conda_env_name}.yaml\n",
    "    else:\n",
    "        # You can rebuild the environment with:\n",
    "        ! conda env update -f ./requirements/{conda_env_name}.yaml --name {conda_env_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b018e6-3147-4123-ab59-40146b498675",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Based on the instructions in the [Parsl Tutorial](https://parsl.readthedocs.io/en/latest/1-parsl-introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576f915-2745-4aef-843e-4aa1986d6130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "# parsl dependencies\n",
    "import parsl\n",
    "import logging\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import Config\n",
    "from parsl.executors import HighThroughputExecutor # We want to use monitoring, so we must use HTEX\n",
    "from parsl.executors import MPIExecutor # MPIExecutor wraps the HTEX; need to test if can be used with monitoring\n",
    "from parsl.monitoring.monitoring import MonitoringHub\n",
    "from parsl.addresses import address_by_hostname\n",
    "from parsl.providers import SlurmProvider, LocalProvider\n",
    "\n",
    "# to display Parsl monitoring GUI in notebook\n",
    "# Experimental - does not work yet\n",
    "from IPython.display import IFrame\n",
    "\n",
    "#=================================================\n",
    "# Log everything to stdout (ends up in pink boxes \n",
    "# in the notebook). This information is logged anyway\n",
    "# in ./runinfo/<run_id>/parsl.log. Careful - this has\n",
    "# the potential to slow the notebook down significantly\n",
    "# for complex workflows.\n",
    "# parsl.set_stream_logger() # <-- log everything to stdout\n",
    "#==================================================\n",
    "\n",
    "print(parsl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6be29-0198-4394-9c9c-94b2f3e10635",
   "metadata": {},
   "source": [
    "## Configure Parsl\n",
    "\n",
    "This configuration must use the `HighThroughputExecutor` (HTEX) since we also want to enable [Parsl monitoring](https://parsl.readthedocs.io/en/latest/userguide/monitoring.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57095f-292c-4b32-ae49-8382ec2fdede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    retries=3,\n",
    "    executors=[\n",
    "        # Use slurm_htex for running SLURM jobs on the worker nodes\n",
    "        HighThroughputExecutor(\n",
    "            label=\"slurm_htex\",\n",
    "            # cores_per_worker is often more general than cores_per_node\n",
    "            # and often allows Parsl scale out multiple Parsl-workers on\n",
    "            # a single worker-node in the most flexible way (i.e. worker\n",
    "            # nodes of different sizes on different CSPs).\n",
    "            cores_per_worker=4,\n",
    "            #max_workers_per_node=3,\n",
    "            address=address_by_hostname(),\n",
    "            provider=SlurmProvider(\n",
    "                partition='big',\n",
    "                nodes_per_block=1,\n",
    "                #cores_per_node=4, # Remove this for now - is one core reserved for the Parsl worker?\n",
    "                init_blocks=2,\n",
    "                min_blocks=2,\n",
    "                max_blocks=10,\n",
    "                exclusive=True,\n",
    "                worker_init=\"source \"+conda_base_path+\"/etc/profile.d/conda.sh; conda activate \"+conda_env_name,\n",
    "                # I don't know why, but Parsl is refusing to relaunch\n",
    "                # Parsl workers via SLURM. Perhaps the default walltime\n",
    "                # of 30 mins is a frim, absolute, limit whereas in the\n",
    "                # past I *think* it has been treated as a per-pilot job limit\n",
    "                # and new workers would get called up as needed. If this\n",
    "                # is a firm limit, then Parsl doesn't know that it's happened:\n",
    "                # once the workers are down, Parsl keeps trying to launch jobs!\n",
    "                walltime=\"01:00:00\")\n",
    "        )\n",
    "    ],\n",
    "    # If this part of the is not present, no \n",
    "    # visualization information is gathered.\n",
    "    monitoring=MonitoringHub(\n",
    "        hub_address=address_by_hostname(),\n",
    "        hub_port=55055,\n",
    "        monitoring_debug=False,\n",
    "        resource_monitoring_interval=10,\n",
    "    ),\n",
    "    strategy='none'\n",
    ")\n",
    "\n",
    "# Loading the configuration starts a Parsl DataFlowKernel\n",
    "# Pilot jobs are also automatically launched at this point \n",
    "# if init_blocks and min_blocks are non-zero. You can\n",
    "# verify this by checking for files in ./runinfo/<run_id>/submit_scripts/\n",
    "# as well as monitoring your SLURM allocation with squeue.\n",
    "dfk = parsl.load(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471719b-de46-473f-8444-295ad8e51914",
   "metadata": {},
   "source": [
    "## Define Parsl apps\n",
    "\n",
    "Parsl workflows are divided into the smallest unit of execution, the app. There are two types of Parsl apps:\n",
    "1. `python_app`s are useful when launching pure Python code. They are also particularly useful if you want to pass a *small* amount of output from a running app directly back into the workflow. For example, `make_dir` below could be set up as a `python_app` or a `bash_app`. But, I choose to make it a `python_app` because I want the value of `my_dir` to be made explicitly available to the workflow via the `.result()` of the app. This is not possible with a `bash_app`.\n",
    "2. `bash_app`s are useful when launching tasks on the command line\n",
    "\n",
    "Here, the applications are *defined* but not run. The `@python_app` and `@bash_app` decorators are the \"flags\" that tell Parsl that these functions are special and need to be tracked as part of the workflow. Undecorated functions execute locally as regular Python in whatever runtime the notebook is in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddeb06-9d92-4290-bb7f-048b1f506ddb",
   "metadata": {},
   "source": [
    "### Python Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c288da-dbd2-49a4-aaad-8a67c472d171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app # make directory to keep all files associated with the ML model\n",
    "def make_dir(my_dir):\n",
    "    import os\n",
    "    os.makedirs(my_dir, exist_ok = False)\n",
    "    return my_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc570-c425-4eda-9c57-73191da7d946",
   "metadata": {},
   "source": [
    "### Bash Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109f82b-c83f-4fcc-8718-6254ed1b5ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app # Start the parsl visulaizer\n",
    "def start_parsl_visualize(\n",
    "    stdout='parsl_vis_app.stdout', \n",
    "    stderr='parsl_vis_app.stderr'):\n",
    "    return 'parsl-visualize --listen 127.0.0.1 --port 8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f69ae-920e-4edb-a66e-c0e04f34cd96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app # Test broadcasting jobs across the worker nodes\n",
    "def worker_hello(enforce, stdout='worker_hello_app.stdout', stderr='worker_hello_app.stdout'):\n",
    "    return '''\n",
    "    date\n",
    "    hn=`hostname`\n",
    "    echo Host: $hn\n",
    "    pwd\n",
    "    whoami\n",
    "    '''\n",
    "\n",
    "@bash_app # Test multiple peer dependencies\n",
    "def count_workers(inputs=(), log_dir=\"./logs\", stdout='count_workers_app.stdout', stderr='count_workers_app.stdout'):\n",
    "    return '''\n",
    "    echo `cat {log_dir}/parsl_hello_app.stdout | col | grep Host | sort | uniq -c`\n",
    "    '''.format(\n",
    "        log_dir=log_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c0f08-efee-4c9e-8191-f6373e7238e4",
   "metadata": {},
   "source": [
    "## Start Parsl monitoring - Option 1 - direct shell invocation to background\n",
    "\n",
    "This step can be done at any point provided that a database file exists.  The default location of this file is in `./runinfo/monitoring.db` and this file is created when the Parsl configuration is loaded. When the notebook kernel is restarted, additional Parsl workflow runs' information is appended to the monitoring information in `./runinfo`. It is possible to view this information \"offline\" (i.e. no active running Parsl workflows) and also fully independently of this notebook (see Option 3, at the end of this notebook for how to specify custom locations fort the monitoring DB). For our purposes here, we will assume that we're using a monitoring DB in the same directory as the notebook runtime in `./runinfo`.\n",
    "\n",
    "This launch can be commented out here since it is also possible to launch `parsl-visualize` from a Parsl app within the workflow, examples of which are below. The advantage to running `parsl-visualize` as a Parsl app is that the visualization server is up and running while the workflow is running and then is shut down when the workflow is cleaned up. Otherwise, when `parsl-visualize` is launched via `os.system` the running child process can persist even after workflow shut down or notebook kernel restart. Here, however, we opt not to use `parsl-visualize` in the workflow because we want only one executor for this workflow for simplicity (to send jobs to the SLURM scheduler) but `parsl-visualize` would need to be started on a local (to the head node) executor.\n",
    "\n",
    "You can rerun this command even if `parsl-visualize` is already running because it checks for existing port usage and if that port is already in use, it fails silently here (on the command line it will give you an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444b703-daee-4ffa-b111-7c7dcd277c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch Parsl \n",
    "os.system('parsl-visualize 1> parsl_vis.stdout 2> parsl_vis.stderr &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3967b-fb75-4cd3-a329-fcce8e3cd523",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parsl Workflow starts here\n",
    "\n",
    "### Make a directory for workflow app logs\n",
    "\n",
    "As part of the workflow, we make a directory for all the app logs. It looks like the first Parsl app to be invoked will start the Parsl interchanges and pilot jobs. This means that you may need to wait some time for this first app to start if you have a queue/cloud spinup wait time associated with getting an allocation of worker nodes **even if this first app is NOT going to worker nodes**!\n",
    "\n",
    "Note that the use of `*_future.result()` blocks the notebook execution (i.e. the cell stays in pending state) until the result of the app future is realized. You can, however, access the state of the future with `*_future` without blocking the workflow. If you don't invoke `*_future.result()`, then the notebook execution contines and builds out the whole Parsl DAG until you reach the first blocking cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf9700-64f9-4893-b875-5cfc5cb4b5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch the app\n",
    "make_log_dir_future = make_dir(param_log_dir)\n",
    "\n",
    "# Get the result\n",
    "log_dir = make_log_dir_future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df7db9-922a-4871-af78-2c475dfdc854",
   "metadata": {},
   "source": [
    "### Start Parsl monitoring - Option 2 - Monitoring as an app within a Parsl workflow\n",
    "\n",
    "This approach is helpful if we want Parsl Monitoring processes to be cleaned up after the workflow is complete. Note that this command is tracked by Parsl and is considered to be part of the workflow. Since we defined the app to use the `local_htex` above, it is running on the head node of the cluster. You can verify this placement in the terminal with `ps -u $USER -HF -ww | grep vis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea30929-87b0-442d-a5aa-ee9cd7a7ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Parsl visualization in a\n",
    "# separate cell since we only want\n",
    "# to run this app one time.\n",
    "#parsl_vis_future = start_parsl_visualize(\n",
    "    #make_log_dir_future, \n",
    "#    stdout=log_dir+'/parsl_vis_app.stdout', \n",
    "#    stderr=log_dir+'/parsl_vis_app.stderr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9acb8-0e41-4ae2-ab7b-20cc9d6eb6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'd love to view Parsl monitoring in the notebook,\n",
    "# but this doesn't work.\n",
    "# IFrame('http://localhost:8080', width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f20000-a71e-4702-b224-590d719504fd",
   "metadata": {},
   "source": [
    "### Example parallel ensemble Parsl workflow\n",
    "\n",
    "The cells below run a very simple parallel workflow example. See below for a workflow running TIEGCM.\n",
    "\n",
    "For development, it's often nice to separate each Parsl app in it's own cell so you can more easily see error messages. Otherwise, error messages can be obscured. Note that the `worker_hello` app is helpful for testing broadcasting an application to many nodes/workers. It is important to ensure that you make a **list** of app futures when launching many Parsl apps - if you overwrite a Parsl future of a still-running application, Parsl tends to get confused and blocks those applications in `launched` state but cannot proceed to update overwritten futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c585d5f-d9e4-4a79-b0f3-210c98daf797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of launching 100 parallel tasks in Parsl\n",
    "# Note that this example is simple but somewhat problematic:\n",
    "# all the Parsl workers will be writing in parallel to the\n",
    "# SAME stdout and stderr. This results in lots of strange\n",
    "# control characters in those two files and even \n",
    "# interrupted/partially overwritten lines -> this operation \n",
    "# is definitely NOT thread safe. BUT, it sure is easy to do!\n",
    "future_list=[]\n",
    "for ii in np.linspace(1,100,100):\n",
    "    future_list.append(\n",
    "        worker_hello(\n",
    "            make_log_dir_future,\n",
    "            stdout=log_dir+'/parsl_hello_app.stdout',\n",
    "            stderr=log_dir+'/parsl_hello_app.stderr'))\n",
    "    \n",
    "\n",
    "count_future = count_workers(\n",
    "    inputs=future_list,\n",
    "    log_dir=log_dir,\n",
    "    stdout=log_dir+'/parsl_count_app.stdout',\n",
    "    stderr=log_dir+'/parsl_count_app.stderr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742f3ea-2d2d-407f-948b-bdb583c15184",
   "metadata": {},
   "source": [
    "## Stop Parsl\n",
    "\n",
    "The cells above can be rerun any number of times; this will simply send more and more apps to be run by Parsl. When the workflow is truly complete, it is time to call the cleanup() command. This command runs implicitly when a `main.py` script finishes executing, but it is *not* run in a notebook unless it is explicitly called as it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2aee4-6d3d-4df8-b2e0-991c10db793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d3a0-3009-472e-b82d-3d4b5566dde1",
   "metadata": {},
   "source": [
    "## Clean up Parsl log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1af19-9196-4a41-b63d-411ebbc52bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This directory contains Parsl monitoring logs\n",
    "! rm -rf runinfo\n",
    "\n",
    "# This directory contains the Parsl app logs\n",
    "! rm -rf {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3341c74-f45a-4851-a744-f12657c29c3b",
   "metadata": {},
   "source": [
    "## Start Parsl Monitoring - Option 3 - Post workflow manual invocation\n",
    "\n",
    "Once the Parsl `./runinfo/monitoring.db` is created, it is possible to start Parsl Monitoring and browse the results of workflow in an offline manner.  In this scenario, `parsl-visualize` can be started on the command line provided that a Conda env with `parsl[visualize]` installed is activated. For example:\n",
    "```\n",
    "source pw/.miniconda3/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "parsl-visualize sqlite:////${HOME}/<work_dir>/runinfo/monitoring.db\n",
    "```\n",
    "(You may need to adjust the path to the Conda environment, its name, and the path to `monitoring.db`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bd202-b336-4f70-888e-d5cba05594d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:parsl]",
   "language": "python",
   "name": "conda-env-parsl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
