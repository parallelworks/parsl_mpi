{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6226ca5-0869-4973-8369-3469cbdff43c",
   "metadata": {},
   "source": [
    "# **CVAE Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dca89-dffa-4a56-8263-371fb372a2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "import papermill as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import netCDF4\n",
    "import cartopy\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8b533-cdaa-4efd-938b-ef1d5a84338e",
   "metadata": {},
   "source": [
    "# Download and Convert Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412796a9-48ba-4a38-b8ae-d1ef35f1fab5",
   "metadata": {},
   "source": [
    "### How we cut down the data\n",
    "\n",
    "1. In the `subset_file` function from the `get_data` library, we used the `ncks -v <variable_name> -d <dim>,<start>,<stop>,<stride> infile.nc -O subset_infile.nc` command to grab every other time step from the original data file. \n",
    "2. In the `load_data` function, we grab the subseted, converted files and concatenate them together to form one dataset before implementing min/max normalization scaling and casting the DataFrame to `float16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08e8b0-f9fa-40b9-852b-8650e681cff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pdir = \"./gefs_data\"\n",
    "data_dir = \"./gefs_data/converted/\"\n",
    "model_dir = './model_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4403c-9c45-4129-902b-93ebfe9749e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data definitions\n",
    "\n",
    "from scripts.get_data import download_file\n",
    "from scripts.get_data import convert_file\n",
    "from scripts.get_data import subset_file\n",
    "from scripts.get_data import remove_data # removes all data\n",
    "\n",
    "# data loading\n",
    "def load_data(data_dir):      \n",
    "    files = [f for f in os.listdir(data_dir) if ('subset' in f and 'tmp' not in f)] # get th list of subseted and converted files from the data directory\n",
    "    \n",
    "    all_data = ((np.expand_dims(\n",
    "        np.concatenate(\n",
    "            [netCDF4.Dataset(data_dir + converted_file)['msl'][:] for converted_file in files] # grab the msl data from each of the files\n",
    "        ),\n",
    "        -1\n",
    "    ).astype(\"float32\") - 85000) / (110000 - 85000)).astype(\"float16\") # min/max normalization scaling and data casting\n",
    "    print(all_data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ebb2a-bc7b-4f52-98d3-ec85c51c7a5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Network Design\n",
    "\n",
    "We need to get to a small latent space. Conv2D networks are good because they help reduce the number of connections in a network in a meaningful way.  I'm using terms as defined in [this definition of conv2D](https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148).\n",
    "\n",
    "**Definitions:**\n",
    "K -> kernel size;\n",
    "P -> padding;\n",
    "S -> stride;\n",
    "D -> Dilation;\n",
    "G -> Groups\n",
    "\n",
    "**Filter options:**\n",
    "Longitude is easy because it is large and even, so as long as you have an even stride, you get integer results when dividing.\n",
    "e.g. lon 9: stride 4, lat 7: stride 5\n",
    "\n",
    "- Latitude - whole numbers occurr for P = 2 & K = 3 or K = 11.\n",
    "- 11 grid points * 0.25 deg * 100 km/deg = 275 km filter window (a good scale for weather)\n",
    "- 9 grid points * 0.25 deg * 100 km/deg = 225 km\n",
    "- Longitude - whole numbers occur for P = 0 & K = 11 (nice match with Latitude), P = 1 & K = 3 or 13, P = 2 & K = 5.\n",
    "\n",
    "For a 5 x 7 filter with 3 stride (no overlap) and no padding:\n",
    "- lat: (721 - 4) / 3 = 239 possible steps (good whole number!)\n",
    "- lon: (1440 - 4) / 3 = 478.6666 possible steps\n",
    "\n",
    "## Load and Preprocess Training Data:\n",
    "The standard way of manipulating arrays in Conv2D layers in TF is to use arrays in the shape:\n",
    "`batch_size,  height, width, channels = data.shape`\n",
    "In our case, the the `batch_size` is the number of image frames (i.e. separate samples or rows in a `.csv` file), the `height` and `width` define the size of the image frame in number of pixels, and the `channels` are the number of layers in the frames.  Typically, channels are color layers (e.g. RGB or CMYK) but in our case, we could use different metereological variables.  However, for this first experiment, **we only need one channel** because we're only going to use mean sea level pressure (msl).\n",
    "\n",
    "## Build the Encoder:\n",
    "GFS grids I have available here are at 0.25 degree resolution.  I'm doing this as a \"worst case\" scenario since there are also 0.5 and 1.0 degree grids with lower resolution but I can't find that data quickly and don't know what's available.\n",
    "\n",
    "These 0.25 degree grids are 721 x 1440.\n",
    "Each forecast file is 3 hourly for 10 days = 8 steps/forecast * 10 days = 80 \"frames\"\n",
    "This demo is only using two forecasts from the control ensemble\n",
    "(one launched Jan 01, 2019 and one launched Jan 02, 2019) -> this is only \n",
    "a small subset of the variability possible in the model.\n",
    "\n",
    "This particular data set spans 2000-2019 and there are 5 ensemble members.\n",
    "\n",
    "## Build the Decoder:\n",
    "With the 11 x 11 and 5 x 5 filters, non-overlapping stride, applied here, we have a final \"image\" size of 14 x 27 and 64 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0144303-f1c9-4220-8178-3988d707688c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model defintions\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=(721, 1440, 1))\n",
    "    \n",
    "    x = layers.Conv2D(32, 11, activation=\"relu\", strides=[9, 10], padding=\"valid\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64, [5,9], activation=\"relu\", strides=[5, 9], padding=\"valid\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    \n",
    "    print(encoder.summary())\n",
    "    return encoder\n",
    "\n",
    "def build_decoder(latent_dim):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(15 * 15 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((15, 15, 64))(x)\n",
    "    # FIXME - there is something wrong here, but at least there is a pattern.\n",
    "    # Using output_padding as a fudge factor -> it may be that there is exactly\n",
    "    # one \"missing\" filter stamp/convolution because for both Conv2DTranspose\n",
    "    # operations, output_padding is set to maximum it could be in both dims\n",
    "    # (i.e. exactly one less than the stride of each filter).\n",
    "    x = layers.Conv2DTranspose(64, [5, 9], activation=\"relu\", strides=[5,9], padding=\"valid\", output_padding=[4, 8])(x)\n",
    "    x = layers.Conv2DTranspose(32, 11, activation=\"relu\", strides=[9,10], padding=\"valid\", output_padding=[8, 9])(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    \n",
    "    print(decoder.summary())\n",
    "    return decoder\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # FIXME: Normalize loss with the number of features (28 * 28)\n",
    "            n_features = 28 * 28\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)\n",
    "                )\n",
    "            ) / n_features\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis = 1)) / n_features\n",
    "            total_loss = (reconstruction_loss + kl_loss)\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    # Needed to validate (validation loss) and to evaluate\n",
    "    def test_step(self, data):\n",
    "        if type(data) == tuple:\n",
    "            data, _ = data\n",
    "            \n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        # FIXME: Normalize loss with the number of features (28 * 28)\n",
    "        n_features = 28 * 28\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction), axis = (1, 2)\n",
    "            )\n",
    "        ) / n_features\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis = 1)) / n_features\n",
    "        total_loss = (reconstruction_loss + kl_loss)\n",
    "        # grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f638f-42a9-4d44-8f31-c6c42017a6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training definitions\n",
    "\n",
    "def train_model(X_train, X_test, X_valid, date, vae, model_dir):\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True) # stops training early if the validation loss does not improve\n",
    "    \n",
    "    if os.path.exists(os.path.join(model_dir, 'vae.weights.h5')): # if the model has already been trained at least once, load that model\n",
    "        vae.load_weights(os.path.join(model_dir, 'vae.weights.h5'))\n",
    "\n",
    "    history = vae.fit(\n",
    "        X_train, epochs = 50, batch_size = 40,\n",
    "        callbacks = [early_stopping_cb],\n",
    "        validation_data = (X_valid,)\n",
    "    )\n",
    "\n",
    "    vae.save_weights(os.path.join(model_dir, 'vae.weights.h5')) # save model weights after training\n",
    "    !cp model_dir/vae.weights.h5 model_dir/vae.weights_{date}.h5 # make a copy to dvc save\n",
    "    \n",
    "    hist_pd = pd.DataFrame(history.history)\n",
    "    hist_pd.to_csv(os.path.join(model_dir, f'history_{date}.csv'), index = False)\n",
    "\n",
    "    test_loss = vae.evaluate(X_test)\n",
    "    test_loss = dict(zip([\"loss\", \"reconstruction_loss\", \"kl_loss\"], test_loss))\n",
    "\n",
    "    print('Test loss:', test_loss)\n",
    "\n",
    "    with open(os.path.join(model_dir, f'test_loss_{date}.json'), 'w') as json_file:\n",
    "        json.dump(test_loss, json_file, indent = 4)\n",
    "        \n",
    "    print(date)\n",
    "    !sh scripts/run_dvcgit.sh model_dir/history_{date}.csv f\"{date}\"\n",
    "    !sh scripts/run_dvcgit.sh model_dir/vae.weights_{date}.h5 f\"{date}\"\n",
    "    !rm model_dir/vae.weights_{date}.h5 # delete copy\n",
    "    \n",
    "def run_train(num_files, date, vae, data_dir, model_dir):\n",
    "    slp = load_data(data_dir) # load data\n",
    "    print(\"shape:\", np.shape(slp)) # verify data shape\n",
    "    \n",
    "    # split the data - y values are throw away\n",
    "    X_train, X_test, y_train, y_test = train_test_split(slp[0:(num_files * 40 - 1), :, :, :], np.arange(0, num_files * 40 - 1), test_size = 0.2, random_state = 1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    train_model(X_train, X_test, X_valid, date, vae, model_dir)\n",
    "    remove_data(data_pdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defdd05-1774-4e56-bd64-2f760c004842",
   "metadata": {},
   "source": [
    "# Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6a16c-2ff9-42fe-bdcd-b8a970f88b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model build\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "# build encoder\n",
    "encoder = build_encoder(latent_dim)\n",
    "print(\"Memory usage after building encoder:\", tf.config.experimental.get_memory_info('GPU:0'))\n",
    "\n",
    "# build decoder\n",
    "decoder = build_decoder(latent_dim)\n",
    "print(\"Memory usage after building decoder:\", tf.config.experimental.get_memory_info('GPU:0'))\n",
    "\n",
    "# build VAE (variational autoencoder)\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer = 'rmsprop') \n",
    "print(\"Memory usage after building VAE:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79096d-dee9-4ffc-9279-442e9232f20f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameter cell for pm \n",
    "year = \"2018\"\n",
    "day = \"01\"\n",
    "months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"] \n",
    "ensembles = [\"c00\"] #, \"p01\", \"p02\", \"p03\", \"p04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952473d-6ec6-4581-8b49-d6d472b4e0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "num_files = 0\n",
    "\n",
    "# get wanted data -------------------------------------------------------------------------\n",
    "for month in months:\n",
    "    for ensemble in ensembles:\n",
    "        download_file(year, month, day, ensemble, data_pdir)\n",
    "        convert_file(year, month, day, ensemble, data_dir)\n",
    "        subset_file(f'pres_msl_{year}{month}{day}00_{ensemble}.nc', data_dir)\n",
    "\n",
    "        if f'pres_msl_{year}{month}{day}00_{ensemble}.nc' in os.listdir(data_dir):\n",
    "            num_files += 1\n",
    "# ------------------------------------------------------------------------------------------          \n",
    "     \n",
    "run_train(num_files, year + day, vae, data_dir, model_dir) # run training \n",
    "    \n",
    "print(\"Memory usage after training:\", tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cvae_env]",
   "language": "python",
   "name": "conda-env-cvae_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
